{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9d2a15eb-28f4-4ae2-913d-dbeb935c2293",
   "metadata": {},
   "source": [
    "### DSC 650\n",
    "### Week 8\n",
    "### Alexis Zimmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ebd978a2-0b2b-4bbf-8953-01806db28c08",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting default log level to \"WARN\".\n",
      "To adjust logging level use sc.setLogLevel(newLevel). For SparkR, use setLogLevel(newLevel).\n",
      "23/05/29 20:30:06 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable\n"
     ]
    }
   ],
   "source": [
    "## import libraries\n",
    "import json\n",
    "import uuid\n",
    "from json import dumps\n",
    "from kafka import KafkaProducer, KafkaAdminClient, KafkaConsumer, TopicPartition\n",
    "from kafka.admin.new_topic import NewTopic\n",
    "from kafka.errors import TopicAlreadyExistsError\n",
    "from kafka.errors import KafkaError\n",
    "from time import sleep\n",
    "import decimal\n",
    "import threading\n",
    "import shutil\n",
    "import os\n",
    "import glob\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.parquet as parq\n",
    "import time\n",
    "import dask.dataframe as dd\n",
    "from pathlib import Path\n",
    "from datetime import datetime, timedelta\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql import SparkSession\n",
    "spark = SparkSession.builder \\\n",
    "    .master('local') \\\n",
    "    .appName('parquetFile') \\\n",
    "    .config('spark.executor.memory', '5gb') \\\n",
    "    .config(\"spark.cores.max\", \"6\") \\\n",
    "    .getOrCreate()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "db219754-88bd-4946-a849-03bae89212fc",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "## Speeds up spark\n",
    "spark.conf.set(\"spark.sql.execution.arrow.pyspark.enabled\", \"true\") \n",
    "spark.conf.set(\"spark.rapids.sql.format.parquet.read.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.rapids.sql.format.parquet.write.enabled\", \"true\")\n",
    "spark.conf.set(\"spark.rapids.sql.format.parquet.reader.type=MULTITHREADED\", \"true\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4e395ee8-da14-4619-b846-7d5ea84aa872",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'bootstrap_servers': ['kafka.kafka.svc.cluster.local:9092'],\n",
       " 'first_name': 'Alexis',\n",
       " 'last_name': 'Zimmer',\n",
       " 'client_id': 'ZimmerAlexis',\n",
       " 'topic_prefix': 'ZimmerAlexis'}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "config = dict(\n",
    "    bootstrap_servers=['kafka.kafka.svc.cluster.local:9092'],\n",
    "    first_name='Alexis',\n",
    "    last_name='Zimmer'\n",
    ")\n",
    "\n",
    "config['client_id'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "config['topic_prefix'] = '{}{}'.format(\n",
    "    config['last_name'], \n",
    "    config['first_name']\n",
    ")\n",
    "\n",
    "config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "913282c7-436a-46b9-929f-a6637ebbc220",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "producer = KafkaProducer(bootstrap_servers=config['bootstrap_servers'], value_serializer=lambda x: json.dumps(x).encode('utf-8'))\n",
    "general_consumer = KafkaConsumer(bootstrap_servers=config['bootstrap_servers'], consumer_timeout_ms=1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "a25dc5fc-b47c-4c12-89b1-feba22baa17f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def loadParquet(parq_path):\n",
    "    pqr = spark.read.parquet(parq_path)\n",
    "    pqr = pqr.toPandas()\n",
    "    return pqr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "d895ed9c-9125-4c46-840e-774a7a037202",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def splitstr(std):\n",
    "    before, after = str(std).split('.')\n",
    "    return before, after"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "7bd44254-d546-4db2-9775-70aff57a486f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def startTimer(results_dir):\n",
    "    print(\"call function\")\n",
    "    retval = startTimedParquetStreamUpdateLoop(results_dir)\n",
    "\n",
    "    if ((time.time() - start_time) < 70 and retval == 0):\n",
    "        t = threading.Timer(interval, startTimer(results_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f935bf5e-6776-44cd-8cc0-c459fc8ddd4a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_kafka_topic(topic_name, config=config, num_partitions=1, replication_factor=1):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    name = '{}-{}'.format(topic_prefix, topic_name)\n",
    "\n",
    "    admin_client = KafkaAdminClient(\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        client_id=client_id\n",
    "    )\n",
    "\n",
    "    topic = NewTopic(\n",
    "        name=name,\n",
    "        num_partitions=num_partitions,\n",
    "        replication_factor=replication_factor\n",
    "    )\n",
    "\n",
    "    topic_list = [topic]\n",
    "    try:\n",
    "        admin_client.create_topics(new_topics=topic_list)\n",
    "        print('Topic Created \"{}\"'.format(name))\n",
    "    except TopicAlreadyExistsError as e:\n",
    "        print('Topic \"{}\" already exists'.format(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "316ced30-fe7d-46e0-910a-50ebae9ad4e8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_kafka_consumer(topics, config=config):\n",
    "    bootstrap_servers = config['bootstrap_servers']\n",
    "    client_id = config['client_id']\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    topic_list = ['{}-{}'.format(topic_prefix, topic) for topic in topics]\n",
    "\n",
    "    return KafkaConsumer(\n",
    "        *topic_list,\n",
    "        client_id=client_id,\n",
    "        bootstrap_servers=bootstrap_servers,\n",
    "        auto_offset_reset='earliest',\n",
    "        enable_auto_commit=False,\n",
    "        value_deserializer=lambda x: json.loads(x)\n",
    "    )\n",
    "\n",
    "consumer = create_kafka_consumer(['locations', 'accelerations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "14d79969-24a9-4c05-b2dd-e79fda024998",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def print_messages(consumer=consumer):\n",
    "    try:\n",
    "\n",
    "        for message in consumer:\n",
    "            msg_metadata = 'Message Metadata: {}:{}:{}'.format(\n",
    "                message.topic, message.partition, message.offset\n",
    "            )\n",
    "\n",
    "            topic = message.topic\n",
    "            tp = TopicPartition(topic, 0)\n",
    "            consumer.seek_to_end(tp)\n",
    "            lastOffset = consumer.position(tp)\n",
    "\n",
    "            if message.key is not None:\n",
    "                msg_key = message.key.decode('utf-8')\n",
    "            else:\n",
    "                msg_key = ''\n",
    "            msg_value = json.dumps(message.value, indent=2)\n",
    "            msg_value = '\\n'.join(['  {}'.format(value) for value in msg_value.split('\\n')])\n",
    "\n",
    "            print('Message Metadata:')\n",
    "            print('  Topic: {}'.format(message.topic))\n",
    "            print('  Partition: {}'.format(message.partition))\n",
    "            print('  Offset: {}'.format(message.offset))\n",
    "            print('Message Key: {}'.format(msg_key))\n",
    "            print('Message Value:')\n",
    "            print(msg_value)\n",
    "            print()\n",
    "            if message.offset == lastOffset - 1:\n",
    "                break\n",
    "\n",
    "    except KeyboardInterrupt:\n",
    "        print(\"Interrupted\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "570040ab-cfe9-48d8-a00a-4b312a3e7dcb",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def on_send_success(record_metadata):\n",
    "    print('Message sent:\\n    Topic: \"{}\"\\n    Partition: {}\\n    Offset: {}'.format(\n",
    "        record_metadata.topic,\n",
    "        record_metadata.partition,\n",
    "        record_metadata.offset\n",
    "    ))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "a58e01d5-b81a-4c17-a290-0aee85ac70a4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def on_send_error(excp):\n",
    "    print('Error', exc_info=excp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "433620b2-4301-46dd-a112-cbd22320c616",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def send_data(topic, data, config=config, producer=producer, msg_key=None):\n",
    "    topic_prefix = config['topic_prefix']\n",
    "    topic_name = '{}-{}'.format(topic_prefix, topic)\n",
    "    print(topic)\n",
    "    print(topic_prefix)\n",
    "    print(topic_name)\n",
    "\n",
    "    if msg_key is not None:\n",
    "        key = msg_key\n",
    "    else:\n",
    "        key = uuid.uuid4().hex\n",
    "\n",
    "    print(data)\n",
    "    sendout = producer.send(topic_name, key=key.encode('utf-8'), value=data).add_callback(on_send_success).add_errback(on_send_error)\n",
    "\n",
    "    try:\n",
    "        record_metadata = sendout.get(timeout=10)\n",
    "    except KafkaError:\n",
    "        log.exception()\n",
    "        pass\n",
    "    \n",
    "    print(record_metadata.topic)\n",
    "    print(record_metadata.partition)\n",
    "    print(record_metadata.offset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "006f9226-10c2-4908-979c-24c3e7f9b653",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Topic Created \"ZimmerAlexis-locations\"\n",
      "Topic Created \"ZimmerAlexis-accelerations\"\n"
     ]
    }
   ],
   "source": [
    "create_kafka_topic('locations')\n",
    "create_kafka_topic('accelerations')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "b070dae5-c682-4175-a575-06aef5c02a65",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "base_dir = Path('C:/Users/21223198/Documents/GitHub/dsc650/assignments/assignment08')\n",
    "results_dir = base_dir.joinpath('results')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5b43c6b9-c6d2-441c-af9d-e6b0c17e69df",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:/Users/21223198/Documents/GitHub/dsc650/data/processed/bdd/accelerations/'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[25], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m fpath \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC:/Users/21223198/Documents/GitHub/dsc650/data/processed/bdd/accelerations/\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m----> 2\u001b[0m targetparqfilenames \u001b[38;5;241m=\u001b[39m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfpath\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m fname \u001b[38;5;129;01min\u001b[39;00m targetparqfilenames:\n\u001b[1;32m      4\u001b[0m     std \u001b[38;5;241m=\u001b[39m (time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time)\n",
      "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:/Users/21223198/Documents/GitHub/dsc650/data/processed/bdd/accelerations/'"
     ]
    }
   ],
   "source": [
    "fpath = 'C:/Users/21223198/Documents/GitHub/dsc650/data/processed/bdd/accelerations/'\n",
    "targetparqfilenames = os.listdir(fpath)\n",
    "for fname in targetparqfilenames:\n",
    "    std = (time.time() - start_time)\n",
    "    before, after = str(std).split('.')\n",
    "    fname_secs = decimal.Decimal(fname.replace(\"t=\",\"\"))\n",
    "    #print(fname_secs)\n",
    "    while std < fname_secs:\n",
    "        sleep(0.05)\n",
    "        std = (time.time() - start_time)\n",
    "        print('Looping til after... ', fname_secs)\n",
    "\n",
    "    print(fname_secs)\n",
    "    parqaccl = 'C:/Users/21223198/Documents/GitHub/dsc650/data/processed/bdd/accelerations/'+str(fname)\n",
    "    parqloc = 'C:/Users/21223198/Documents/GitHub/dsc650/data/processed/bdd/locations/'+str(fname)\n",
    "\n",
    "    if fname_secs == 52.5:\n",
    "        # Producer\n",
    "        print(\"At the 52.5 mark\")\n",
    "        par_accelerations = loadParquet(parqaccl)\n",
    "        par_accelerations = par_accelerations.to_json()\n",
    "        par_locations = loadParquet(parqloc)\n",
    "        par_locations = par_locations.to_json()\n",
    "        send_data('accelerations', par_accelerations)\n",
    "        send_data('locations', par_locations)\n",
    "        break\n",
    "    print(\"Finished!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aee728c0-8496-42e8-959e-bcd30bd4ef94",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
